import numpy as np
import torch
from dqn_env import FruitBoxEnv
from dqn_agent import DQNAgent
from utils.group_generator import generate_all_valid_groups
import matplotlib.pyplot as plt

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
EPISODES = 1000
TARGET_SCORE = 160
ACTION_SAMPLE_LIMIT = 100  # ìµœëŒ€ í–‰ë™ ìˆ˜ ì œí•œ

# ì´ˆê¸° ìƒíƒœ ë¶ˆëŸ¬ì˜¤ê¸°
board = np.loadtxt("data/ocr_result_corrected_10x17.csv", delimiter=",", dtype=int)
env = FruitBoxEnv(board)

# ì´ˆê¸° ìœ íš¨ ê·¸ë£¹
action_list = env.get_action_list()
print(f"ğŸ¯ ê°€ëŠ¥í•œ ê·¸ë£¹ ê°œìˆ˜: {len(action_list)}")

# ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
agent = DQNAgent(action_count=len(action_list))

# ê¸°ë¡ìš©
rewards_log = []
epsilon_log = []

# í•™ìŠµ ë£¨í”„
for episode in range(EPISODES):
    state = env.reset()
    total_reward = 0
    steps = 0

    while True:
        action_list = env.get_action_list()
        if len(action_list) == 0:
            break

        # í˜„ì¬ action_listë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¸ë±ìŠ¤ ì„ íƒ
        action_idx = agent.select_action(state)
        action_idx = min(action_idx, len(action_list) - 1)  # out of range ë°©ì§€
        group = action_list[action_idx]

        next_state, reward, done = env.step(group)

        agent.remember(state, action_idx, reward, next_state, done)
        agent.train()

        state = next_state
        total_reward += reward
        steps += 1

        if done or steps >= ACTION_SAMPLE_LIMIT:
            break

    agent.update_target_network()
    rewards_log.append(total_reward)
    epsilon_log.append(agent.epsilon)

    print(f"[EP {episode+1}] ì´ ë³´ìƒ: {total_reward}, ë‹¨ê³„ ìˆ˜: {steps}, epsilon: {agent.epsilon:.4f}")

    if total_reward >= TARGET_SCORE:
        print("ğŸ‰ ëª©í‘œ ì ìˆ˜ ë„ë‹¬! í•™ìŠµ ì¡°ê¸° ì¢…ë£Œ")
        break

# âœ… ëª¨ë¸ ì €ì¥
model_path = "model/conv_dqn_weights.pth"
torch.save(agent.model.state_dict(), model_path)
print(f"âœ… í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")

# ğŸ“ˆ ì‹œê°í™”
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(rewards_log)
plt.title("Episode Reward")
plt.xlabel("Episode")
plt.ylabel("Total Reward")

plt.subplot(1, 2, 2)
plt.plot(epsilon_log)
plt.title("Epsilon Decay")
plt.xlabel("Episode")
plt.ylabel("Epsilon")

plt.tight_layout()
plt.savefig("training_progress.png")
print("ğŸ“Š í•™ìŠµ ê³¼ì • ì‹œê°í™” ì €ì¥ ì™„ë£Œ: training_progress.png")