import numpy as np
import time
import os
from dqn_env import FruitBoxEnv
from dqn_agent import DQNAgent
from utils.group_generator import generate_all_valid_groups
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import cv2
import torch
import matplotlib.pyplot as plt

# ğŸ“Œ í¬ë¡¬ ë“œë¼ì´ë²„ ê²½ë¡œ
chrome_driver_path = "chromedriver-win64/chromedriver.exe"

# ğŸ“Œ í¬ë¡¬ ì˜µì…˜ ì„¤ì •
chrome_options = Options()
chrome_options.add_argument("--ignore-certificate-errors")
chrome_options.add_argument("--ignore-ssl-errors")

# ğŸ“Œ ë“œë¼ì´ë²„ ì‹¤í–‰
service = Service(chrome_driver_path)
driver = webdriver.Chrome(service=service, options=chrome_options)

# ğŸ“Œ ê²Œì„ í˜ì´ì§€ ì—´ê¸°
driver.get("https://www.gamesaien.com/game/fruit_box_a/")

# ğŸ“Œ ìº”ë²„ìŠ¤ ëŒ€ê¸° ë° ìœ„ì¹˜ ê³„ì‚°
time.sleep(2)
canvas = driver.find_element(By.ID, "canvas")
canvas_rect = driver.execute_script("""
    const rect = document.getElementById('canvas').getBoundingClientRect();
    return {left: rect.left, top: rect.top, width: rect.width, height: rect.height};
""")
canvas_left = int(canvas_rect["left"])
canvas_top = int(canvas_rect["top"])
canvas_width = int(canvas_rect["width"])
canvas_height = int(canvas_rect["height"])

# ğŸ“Œ Play ë²„íŠ¼ í´ë¦­
play_x = canvas_left + (canvas_width // 2) - 170
play_y = canvas_top + (canvas_height // 2)
driver.execute_script(f"""
    const canvas = document.getElementById('canvas');
    const rect = canvas.getBoundingClientRect();
    const x = rect.left + {play_x - canvas_left};
    const y = rect.top + {play_y - canvas_top};
    ['mousedown', 'mouseup'].forEach(type => {{
        canvas.dispatchEvent(new MouseEvent(type, {{
            bubbles: true, cancelable: true, view: window,
            clientX: x, clientY: y
        }}));
    }});
""")
print(f"âœ… 'Play' ë²„íŠ¼ í´ë¦­ ì™„ë£Œ! ìœ„ì¹˜: ({play_x}, {play_y})")

# ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥
time.sleep(2)
canvas.screenshot("data/game_screenshot.png")
print("ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì™„ë£Œ: game_screenshot.png")

# ğŸ” ì‚¬ê³¼ ì¤‘ì‹¬ ì¢Œí‘œ ê²€ì¶œ
image = cv2.imread("data/game_screenshot.png")
hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
lower_red1, upper_red1 = np.array([0, 100, 100]), np.array([10, 255, 255])
lower_red2, upper_red2 = np.array([160, 100, 100]), np.array([179, 255, 255])
mask1 = cv2.inRange(hsv, lower_red1, upper_red1)
mask2 = cv2.inRange(hsv, lower_red2, upper_red2)
red_mask = cv2.bitwise_or(mask1, mask2)
kernel = np.ones((3, 3), np.uint8)
clean_mask = cv2.morphologyEx(red_mask, cv2.MORPH_OPEN, kernel, iterations=2)
clean_mask = cv2.dilate(clean_mask, kernel, iterations=1)

contours, _ = cv2.findContours(clean_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
circle_centers = []
for cnt in contours:
    if cv2.contourArea(cnt) < 50:
        continue
    (x, y), radius = cv2.minEnclosingCircle(cnt)
    if 8 <= radius <= 30:
        circle_centers.append((int(x), int(y)))

if len(circle_centers) >= 170:
    sorted_centers = sorted(circle_centers, key=lambda c: (c[1], c[0]))[:170]
    center_grid = np.array(sorted_centers, dtype=np.int32).reshape((10, 17, 2))
    np.save("data/center_grid.npy", center_grid)
    print("âœ… ì‚¬ê³¼ ì¤‘ì‹¬ ì¢Œí‘œ ì €ì¥ ì™„ë£Œ (center_grid.npy)")
else:
    print(f"âš ï¸ ê°ì§€ëœ ì¤‘ì‹¬ ë¶€ì¡±: {len(circle_centers)}ê°œ")
    driver.quit()
    exit()

# ğŸ”  OCR ì‹¤í–‰
os.system("python ocr_pipline.py")
board = np.loadtxt("data/ocr_result_corrected_10x17.csv", delimiter=",", dtype=int)

# ğŸ¯ DQN í•™ìŠµ ì‹œì‘
from dqn_env import FruitBoxEnv
from dqn_agent import DQNAgent

EPISODES = 1
ACTION_SAMPLE_LIMIT = 100
TARGET_SCORE = 160

env = FruitBoxEnv(board)
action_list = env.get_action_list()
agent = DQNAgent(action_count=max(len(action_list), 1))

rewards_log = []
epsilon_log = []

for episode in range(EPISODES):
    state = env.reset()
    total_reward = 0
    steps = 0

    while True:
        action_list = env.get_action_list()
        if len(action_list) == 0:
            break

        action_idx = agent.select_action(state)
        action_idx = min(action_idx, len(action_list) - 1)
        group = action_list[action_idx]

        next_state, reward, done = env.step(group)

        agent.remember(state, action_idx, reward, next_state, done)
        agent.train()

        state = next_state
        total_reward += reward
        steps += 1

        if done or steps >= ACTION_SAMPLE_LIMIT:
            break

    agent.update_target_network()
    rewards_log.append(total_reward)
    epsilon_log.append(agent.epsilon)

    print(f"[EP {episode+1}] ì´ ë³´ìƒ: {total_reward}, ë‹¨ê³„ ìˆ˜: {steps}, epsilon: {agent.epsilon:.4f}")

    if total_reward >= TARGET_SCORE:
        print("ğŸ‰ ëª©í‘œ ì ìˆ˜ ë„ë‹¬! í•™ìŠµ ì¡°ê¸° ì¢…ë£Œ")
        break

# âœ… ëª¨ë¸ ì €ì¥
model_path = "model/conv_dqn_weights.pth"
torch.save(agent.model.state_dict(), model_path)
print(f"âœ… í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")

# ğŸ“Š ì‹œê°í™”
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(rewards_log)
plt.title("Episode Reward")
plt.xlabel("Episode")
plt.ylabel("Total Reward")

plt.subplot(1, 2, 2)
plt.plot(epsilon_log)
plt.title("Epsilon Decay")
plt.xlabel("Episode")
plt.ylabel("Epsilon")

plt.tight_layout()
plt.savefig("data/training_progress.png")
print("ğŸ“ˆ í•™ìŠµ ì‹œê°í™” ì €ì¥ ì™„ë£Œ: training_progress.png")

input("ê²Œì„ ì¢…ë£Œ í›„ Enterë¥¼ ëˆŒëŸ¬ ë¸Œë¼ìš°ì €ë¥¼ ë‹«ìŠµë‹ˆë‹¤...")
driver.quit()